{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "# model = keras.models.load_model('m.h5')\n",
    "# customModel = KerasClassifier(model = model, epochs=100, batch_size=16, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commentCleaner(comments):\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "        comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "        comment = comment.lower()\n",
    "        # Tokenize the comment\n",
    "        tokens = comment.split()\n",
    "        # tokens = comment.split(' ')\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Join the tokens back into a single string\n",
    "        cleaned_comment = \" \".join(tokens)\n",
    "        cleaned_comments.append(cleaned_comment)\n",
    "        \n",
    "    \n",
    "    return cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>geosolutions technology leverage benefon gps s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>esi low 150 250 bk real possibility</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>last quarter 2010 componenta net sale doubled ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>according finnishrussian chamber commerce majo...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>swedish buyout firm sold remaining 224 percent...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence Sentiment\n",
       "0           0  geosolutions technology leverage benefon gps s...  positive\n",
       "1           1                esi low 150 250 bk real possibility  negative\n",
       "2           2  last quarter 2010 componenta net sale doubled ...  positive\n",
       "3           3  according finnishrussian chamber commerce majo...   neutral\n",
       "4           4  swedish buyout firm sold remaining 224 percent...   neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"augmented_data.csv\")\n",
    "cleaned_data = data.copy()\n",
    "cleaned_data[\"Sentence\"] = commentCleaner(cleaned_data[\"Sentence\"])\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   negative  neutral  positive\n",
      "0     False    False      True\n",
      "1      True    False     False\n",
      "2     False    False      True\n",
      "3     False     True     False\n",
      "4     False     True     False\n",
      "   Unnamed: 0                                           Sentence Sentiment\n",
      "0           0  The GeoSolutions technology will leverage Bene...  positive\n",
      "1           1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
      "2           2  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "3           3  According to the Finnish-Russian Chamber of Co...   neutral\n",
      "4           4  The Swedish buyout firm has sold its remaining...   neutral\n",
      "      negative  neutral  positive\n",
      "3208     False    False      True\n",
      "4343     False     True     False\n",
      "2153     False     True     False\n",
      "5551      True    False     False\n",
      "6571      True    False     False\n",
      "['geosolutions technology leverage benefon gps solution providing location based search technology community platform location relevant multimedia content new powerful commercial model'\n",
      " 'esi low 150 250 bk real possibility'\n",
      " 'last quarter 2010 componenta net sale doubled eur131m eur76m period year earlier moved zero pretax profit pretax loss eur7m'\n",
      " ... 'operating profit savage eur 381 mn eur 553 mn 2007'\n",
      " 'hsbc articulate unit book 585 million charge settlement'\n",
      " 'rising cost forced packaging producer axe 90 job hampshire manufacturing plant']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3208    - I am pleased that Bjorn Wahlroos has accepte...\n",
       "4343    Auburn 's sales in 2007 were CAD 41 million ( ...\n",
       "2153    Profit for the period totalled EUR 1.1 mn , do...\n",
       "5551    $ACOM http://stks.co/1G6x Downside breakout lo...\n",
       "6571    78 users on Vetr are bearish on Tesla Motors, ...\n",
       "                              ...                        \n",
       "3772    @BULLYA @pollux654321 My 50 $KORS 80 Calls are...\n",
       "5191    According to Sepp+Ã±nen , the new technology UM...\n",
       "5226     $CRUS Upgraded to a buy by Alpha Street Research\n",
       "5390    Favourable currency rates also contributed to ...\n",
       "860     Tesco breaks its downward slide by cutting sal...\n",
       "Name: Sentence, Length: 4635, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the integer labels to onehot encoding]\n",
    "data = pd.read_csv('augmented_data.csv')\n",
    "y = pd.get_dummies(data['Sentiment'])\n",
    "print(y.head())\n",
    "X = data[\"Sentence\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "print(data.head())\n",
    "print(y_train.head())\n",
    "tokenizerFinal = Tokenizer(num_words=1000, split=' ') \n",
    "print(cleaned_data['Sentence'].values)\n",
    "tokenizerFinal.fit_on_texts(cleaned_data['Sentence'].values)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['- I am pleased that Bjorn Wahlroos has accepted the nomination .'\n \"Auburn 's sales in 2007 were CAD 41 million ( approximately EUR 27 million ) , and the company employs some 150 people .\"\n 'Profit for the period totalled EUR 1.1 mn , down from EUR 1.6 mn in the third quarter of 2008 .'\n ... '$CRUS Upgraded to a buy by Alpha Street Research'\n \"Favourable currency rates also contributed to higher net sales , '' CEO Kari Kauniskangas said .\"\n 'Tesco breaks its downward slide by cutting sales decline in half'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m customModel\u001b[39m.\u001b[39;49minitialize(X_train, y_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/scikeras/wrappers.py:1455\u001b[0m, in \u001b[0;36mKerasClassifier.initialize\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize the model without any fitting.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \u001b[39mYou only need to call this model if you explicitly do not want to do any fitting\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \u001b[39m(for example with a pretrained model). You should _not_ call this\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39m    A reference to the KerasClassifier instance for chained calling.\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1455\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49minitialize(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my)\n\u001b[1;32m   1456\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/scikeras/wrappers.py:878\u001b[0m, in \u001b[0;36mBaseWrapper.initialize\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBaseWrapper\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    858\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize the model without any fitting.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m    You only need to call this model if you explicitly do not want to do any fitting\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[39m        A reference to the BaseWrapper instance for chained calling.\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(X, y)\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/scikeras/wrappers.py:843\u001b[0m, in \u001b[0;36mBaseWrapper._initialize\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     \u001b[39m# int or None\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state\n\u001b[0;32m--> 843\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_encoder_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_encoder\u001b[39m.\u001b[39mfit(y)\n\u001b[1;32m    846\u001b[0m target_metadata \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_encoder_, \u001b[39m\"\u001b[39m\u001b[39mget_metadata\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mdict\u001b[39m)()\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/scikeras/wrappers.py:616\u001b[0m, in \u001b[0;36mBaseWrapper._validate_data\u001b[0;34m(self, X, y, reset, y_numeric)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mfloatx()\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 616\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(\n\u001b[1;32m    617\u001b[0m         X,\n\u001b[1;32m    618\u001b[0m         y,\n\u001b[1;32m    619\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    620\u001b[0m         allow_nd\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# allow X to have more than 2 dimensions\u001b[39;49;00m\n\u001b[1;32m    621\u001b[0m         multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# allow y to be 2D\u001b[39;49;00m\n\u001b[1;32m    622\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    627\u001b[0m         y,\n\u001b[1;32m    628\u001b[0m         ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m         allow_nd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    630\u001b[0m         dtype\u001b[39m=\u001b[39m_check_array_dtype(y, force_numeric\u001b[39m=\u001b[39my_numeric),\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    947\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    949\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['- I am pleased that Bjorn Wahlroos has accepted the nomination .'\n \"Auburn 's sales in 2007 were CAD 41 million ( approximately EUR 27 million ) , and the company employs some 150 people .\"\n 'Profit for the period totalled EUR 1.1 mn , down from EUR 1.6 mn in the third quarter of 2008 .'\n ... '$CRUS Upgraded to a buy by Alpha Street Research'\n \"Favourable currency rates also contributed to higher net sales , '' CEO Kari Kauniskangas said .\"\n 'Tesco breaks its downward slide by cutting sales decline in half'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "customModel.initialize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__builtin__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m pipeline \u001b[39m=\u001b[39m load(\u001b[39m'\u001b[39;49m\u001b[39m/Users/aadeesh/redditSentiment/pipeline.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mm.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[1;32m    659\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1213\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/pickle.py:1528\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1526\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1527\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1528\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name)\n\u001b[1;32m   1529\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(klass)\n",
      "File \u001b[0;32m~/miniconda3/envs/redditEnv/lib/python3.9/pickle.py:1579\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[39melif\u001b[39;00m module \u001b[39min\u001b[39;00m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING:\n\u001b[1;32m   1578\u001b[0m         module \u001b[39m=\u001b[39m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[0;32m-> 1579\u001b[0m \u001b[39m__import__\u001b[39;49m(module, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39mmodules[module], name)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__builtin__'"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "from keras.models import load_model\n",
    "pipeline = load('/Users/aadeesh/redditSentiment/pipeline.pkl')\n",
    "pipeline.named_steps['model'].model = load_model('m.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "def classification_model():\n",
    "    # Building our model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(1000, 120, input_length = 31))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = classification_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one reviewer mentioned watching 1 oz episode']\n"
     ]
    }
   ],
   "source": [
    "# Cleans the sentence of links and emojis, removes stop words, and lemmatizes words\n",
    "def commentCleaner(comments):\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "        comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "        comment = comment.lower()\n",
    "        # Tokenize the comment\n",
    "        tokens = comment.split()\n",
    "        # tokens = comment.split(' ')\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Join the tokens back into a single string\n",
    "        cleaned_comment = \" \".join(tokens)\n",
    "        cleaned_comments.append(cleaned_comment)\n",
    "        \n",
    "    \n",
    "    return cleaned_comments\n",
    "\n",
    "print(commentCleaner([\"One of the other reviewers mentioned watching 1 oz episode\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeComments(comments, tokenizer):\n",
    "    print(\"Comments recieved for tokenization: \")\n",
    "    print(comments)\n",
    "    # tokenizer = Tokenizer(num_words=1000, split=' ') \n",
    "    # tokenizer.fit_on_texts(comments)\n",
    "    print(\"Fitted tokenizer to sample texts\")\n",
    "    tokenized_comments = tokenizer.texts_to_sequences(comments)\n",
    "    print(\"Converted to sequences\")\n",
    "    tokenized_comments = pad_sequences(tokenized_comments, 31)\n",
    "    print(\"Padded succesfully\")\n",
    "    print(tokenized_comments)\n",
    "    return tokenized_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "def load_pipeline_keras(cleaner, model, tokenizer, folder_name=\"model\"):\n",
    "    cleaner = pickle.load(open(folder_name+'/'+cleaner,'rb'))\n",
    "    tokenizerFinal = pickle.load(open(folder_name+'/'+tokenizer,'rb'))\n",
    "    build_model = keras.models.load_model(folder_name+'/'+model)\n",
    "    cleaner.tokenizer = tokenizerFinal\n",
    "    # classifier = KerasClassifier(model=build_model, epochs=1, batch_size=10, verbose=1)\n",
    "    # classifier.classes_ = pickle.load(open(folder_name+'/'+classes,'rb'))\n",
    "    # classifier.model = build_model\n",
    "    # build_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return Pipeline([\n",
    "        ('cleaner', cleaner),\n",
    "        ('lstm', build_model)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;cleaner&#x27;,\n",
       "                 textTransformer(tokenizer=&lt;keras.src.preprocessing.text.Tokenizer object at 0x151fcab20&gt;)),\n",
       "                (&#x27;lstm&#x27;,\n",
       "                 &lt;keras.src.engine.sequential.Sequential object at 0x151faf580&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cleaner&#x27;,\n",
       "                 textTransformer(tokenizer=&lt;keras.src.preprocessing.text.Tokenizer object at 0x151fcab20&gt;)),\n",
       "                (&#x27;lstm&#x27;,\n",
       "                 &lt;keras.src.engine.sequential.Sequential object at 0x151faf580&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">textTransformer</label><div class=\"sk-toggleable__content\"><pre>textTransformer(tokenizer=&lt;keras.src.preprocessing.text.Tokenizer object at 0x151fcab20&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Sequential</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.src.engine.sequential.Sequential object at 0x151faf580&gt;</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('cleaner',\n",
       "                 textTransformer(tokenizer=<keras.src.preprocessing.text.Tokenizer object at 0x151fcab20>)),\n",
       "                ('lstm',\n",
       "                 <keras.src.engine.sequential.Sequential object at 0x151faf580>)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = load_pipeline_keras('transformer.pkl', 'lstm.h5', 'tokenizer.pkl')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform\n",
      "['Well Apple actually makes quality products as opposed to those others']\n",
      "Cleaned comments\n",
      "Starting tokenization\n",
      "Comments recieved for tokenization: \n",
      "['well apple actually make quality product opposed others']\n",
      "Fitted tokenizer to sample texts\n",
      "Converted to sequences\n",
      "Padded succesfully\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  76 659 186 745  37]]\n",
      "Tokenized\n",
      "Ending transform\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x151fc5ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "[[0.16205938 0.6267933  0.21114734]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = pipeline.predict(['Well Apple actually makes quality products as opposed to those others'])\n",
    "\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cleaner',\n",
       "  textTransformer(tokenizer=<keras.src.preprocessing.text.Tokenizer object at 0x299992be0>)),\n",
       " ('lstm',\n",
       "  KerasClassifier(\n",
       "  \tmodel=<keras.src.engine.sequential.Sequential object at 0x299a01940>\n",
       "  \tbuild_fn=<function load_pipeline_keras.<locals>.<lambda> at 0x2999971f0>\n",
       "  \twarm_start=False\n",
       "  \trandom_state=None\n",
       "  \toptimizer=rmsprop\n",
       "  \tloss=None\n",
       "  \tmetrics=None\n",
       "  \tbatch_size=10\n",
       "  \tvalidation_batch_size=None\n",
       "  \tverbose=1\n",
       "  \tcallbacks=None\n",
       "  \tvalidation_split=0.0\n",
       "  \tshuffle=True\n",
       "  \trun_eagerly=False\n",
       "  \tepochs=1\n",
       "  \tclass_weight=None\n",
       "  ))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
