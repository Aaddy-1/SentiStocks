{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66457"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv(\"/Users/aadeesh/redditSentiment/server/Data/redditData/Posts/post.csv\")\n",
    "len(comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional, Flatten, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def classification_model():\n",
    "    # Building our model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(18364, 256, input_length = 235))\n",
    "    model.add(SpatialDropout1D(0.5))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=128, dropout=0.6)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "checkpoint_path = \"final1/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# Create a ModelCheckpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback to stop training if validation loss doesn't improve\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,  # Number of epochs with no improvement after which training will stop\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "class customModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, batch_size):\n",
    "        self.model_fn = classification_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.model_fn\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.model.fit(X, y, epochs = 7, batch_size=self.batch_size, callbacks = [checkpoint_callback, early_stopping_callback], verbose = 1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "def commentCleaner(comments):\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "        comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "        comment = comment.lower()\n",
    "        # Tokenize the comment\n",
    "        tokens = comment.split()\n",
    "        # tokens = comment.split(' ')\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Join the tokens back into a single string\n",
    "        cleaned_comment = \" \".join(tokens)\n",
    "        cleaned_comments.append(cleaned_comment)   \n",
    "    return cleaned_comments\n",
    "\n",
    "\n",
    "    \n",
    "def tokenizeComments(comments, tokenizer):\n",
    "    # print(\"Comments recieved for tokenization: \")\n",
    "    # print(comments)\n",
    "    # print(\"Fitted tokenizer to sample texts\")\n",
    "    tokenized_comments = tokenizer.texts_to_sequences(comments)\n",
    "    # print(\"Converted to sequences\")\n",
    "    tokenized_comments = pad_sequences(tokenized_comments, 235)\n",
    "    # print(\"Padded succesfully\")\n",
    "    # print(tokenized_comments)\n",
    "    return tokenized_comments\n",
    "\n",
    "class textTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # print(\"Starting fitting\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # print(\"Starting transform\")\n",
    "        # print(X)\n",
    "        # tokenizerFinal = Tokenizer(num_words=1000, split=' ') \n",
    "        # print(cleaned_data['Sentence'].values)\n",
    "        # tokenizerFinal.fit_on_texts(cleaned_data['Sentence'].values)\n",
    "        X_cleaned = commentCleaner(X)\n",
    "        # print(\"Cleaned comments\")\n",
    "        # print(\"Starting tokenization\")\n",
    "        X_tokenized = tokenizeComments(X_cleaned, self.tokenizer)\n",
    "        # print(\"Tokenized\")\n",
    "        # print(\"Ending transform\")\n",
    "\n",
    "        return X_tokenized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "def load_pipeline_keras(cleaner, model, tokenizer, folder_name=\"model\"):\n",
    "    cleaner = pickle.load(open(cleaner,'rb'))\n",
    "    tokenizerFinal = pickle.load(open(tokenizer,'rb'))\n",
    "    model = keras.models.load_model(model)\n",
    "    cleaner.tokenizer = tokenizerFinal\n",
    "    # classifier = KerasClassifier(model=build_model, epochs=1, batch_size=10, verbose=1)\n",
    "    # classifier.classes_ = pickle.load(open(folder_name+'/'+classes,'rb'))\n",
    "    # classifier.model = build_model\n",
    "    # build_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return Pipeline([\n",
    "        ('textTransformer', cleaner),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    classifier = load_pipeline_keras('/Users/aadeesh/redditSentiment/server/model/classifier/textTransformer.pkl', \n",
    "                    '/Users/aadeesh/redditSentiment/server/model/classifier/model.h5', \n",
    "                    '/Users/aadeesh/redditSentiment/server/model/classifier/tokenizer.pkl', \n",
    "                    'server/model/classifier')\n",
    "    return classifier\n",
    "\n",
    "classifier = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframeProcessor(df, classifier):\n",
    "\n",
    "    keywords = {\"Tesla\" : [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\"],\n",
    "            \"Apple\" : [\"$aapl\", \"aapl\", \"apple\", \"mac\", \"iphone\", \"airpods\", \"macbook\"], \n",
    "            \"Nvidia\" : [\"$nvda\", \"nvda\", \"nvidia\", \"rtx\", \"geforce\", \"jensen\", \"huang\"], \n",
    "            \"Google\" : [\"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\", \"android\", \"pixel\", \"sundar pichai\", \"sundar\", \"pichai\"],\n",
    "            \"Amazon\" : [\"$amzn\", \"amzn\", \"amazon\", \"aws\", \"prime\", \"alexa\", \"fire tv\", \"amazon prime\"],\n",
    "            \"Microsoft\" : [\"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\", \"xbox\"],\n",
    "            \"Meta\" : [\"$meta\", \"meta\", \"instagram\", \"facebook\", \"threads\"]\n",
    "        }\n",
    "    keywords2 = [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\", \n",
    "             \"$aapl\", \"aapl\", \"apple\", \"mac\", \"iphone\", \"airpods\", \"macbook\"\n",
    "             \"$nvda\", \"nvda\", \"nvidia\", \"rtx\", \"geforce\", \"jensen huang\", \"jensen\", \"huang\" \n",
    "             \"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\", \"android\", \"pixel\", \"sundar pichai\", \"sundar\", \"pichai\"\n",
    "             \"$amzn\", \"amzn\", \"amazon\", \"aws\", \"prime\", \"alexa\", \"fire tv\", \"amazon prime\"\n",
    "             \"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\", \"xbox\"\n",
    "             \"$meta\", \"meta\", \"instagram\", \"facebook\", \"threads\"\n",
    "        ]\n",
    "\n",
    "    filtered_df = df[df['Comment'].str.contains('|'.join(keywords2), case = False)]\n",
    "\n",
    "    # Add an extra column to the filtered dataframe that indicates which keyword was present in that comment\n",
    "    def keyWordBuilder(comment):\n",
    "        returnString = \"\"\n",
    "        for keyword in keywords2:\n",
    "            if keyword in comment.lower():\n",
    "                for key in keywords:\n",
    "                    if keyword in keywords[key]:\n",
    "                        if key not in returnString:\n",
    "                            returnString += key + ' '\n",
    "        if returnString == \"\":\n",
    "            return \"None\"\n",
    "        return returnString\n",
    "\n",
    "    keyWordList = filtered_df['Comment'].apply(keyWordBuilder)\n",
    "\n",
    "    filtered_df = filtered_df.assign(Keyword = keyWordList)\n",
    "\n",
    "    newDates = pd.to_datetime(filtered_df['Date'])\n",
    "    newDates = newDates.dt.date\n",
    "    filtered_df = filtered_df.assign(Date = newDates)\n",
    "    filtered_df = filtered_df.sort_values(by='Date', ascending=True)\n",
    "\n",
    "    comments = filtered_df.Comment\n",
    "    preds = classifier.predict(comments)\n",
    "\n",
    "    sentiments = np.argmax(preds, axis = 1)\n",
    "    # preds\n",
    "\n",
    "    filtered_df = filtered_df.assign(Sentiment = sentiments)\n",
    "\n",
    "    return filtered_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonBuilder(filtered_df):\n",
    "    # filtered_rows = filtered_df[filtered_df['Keyword'].str.contains('tesla', case=False)]\n",
    "    # filtered_rows['Date'] = pd.to_datetime(filtered_rows['Date'])\n",
    "\n",
    "    # # Extract only the date part from the 'Date' column\n",
    "    # filtered_rows['Date'] = filtered_rows['Date'].dt.date\n",
    "    # print(filtered_rows.head())\n",
    "    tesla_df, apple_df, nvda_df, google_df, amzn_df, msft_df, meta_df = {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    done = []\n",
    "    for i in (filtered_df.Date):\n",
    "        date_string = i.strftime('%m-%d')\n",
    "        if date_string not in done:\n",
    "            tesla_df[date_string] = 0\n",
    "            apple_df[date_string] = 0\n",
    "            nvda_df[date_string] = 0\n",
    "            google_df[date_string] = 0\n",
    "            amzn_df[date_string] = 0\n",
    "            msft_df[date_string] = 0\n",
    "            meta_df[date_string] = 0\n",
    "        done.append(date_string)\n",
    "\n",
    "    for i, j, k in zip(filtered_df.Date, filtered_df.Keyword, filtered_df.Sentiment):\n",
    "        date_string = i.strftime('%m-%d')\n",
    "        val = 1\n",
    "        if k == 0:\n",
    "            val = 0\n",
    "        for keyword in j.split():\n",
    "            if keyword == \"Tesla\":\n",
    "                tesla_df[date_string] += val\n",
    "            if keyword == \"Apple\":\n",
    "                apple_df[date_string] += val\n",
    "            if keyword == \"Nvidia\":\n",
    "                nvda_df[date_string] += val\n",
    "            if keyword == \"Google\":\n",
    "                google_df[date_string] += val\n",
    "            if keyword == \"Amazon\":\n",
    "                amzn_df[date_string] += val\n",
    "            if keyword == \"Microsoft\":\n",
    "                msft_df[date_string] += val\n",
    "            if keyword == \"Meta\":\n",
    "                meta_df[date_string] += val\n",
    "    return [tesla_df, apple_df, nvda_df, google_df, amzn_df, msft_df, meta_df]\n",
    "\n",
    "# l = jsonBuilder(filtered_df=filtered_df)\n",
    "# for i in l:\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 22s 102ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Length</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>14hw652</td>\n",
       "      <td>LOST 11k in four days and blew my account.</td>\n",
       "      <td>2023-06-24</td>\n",
       "      <td>All ya had to do was put it in Tesla and Amazo...</td>\n",
       "      <td>62</td>\n",
       "      <td>Tesla Amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40781</th>\n",
       "      <td>14hs93o</td>\n",
       "      <td>Jensen Huang finally sold some NVDA stock.</td>\n",
       "      <td>2023-06-24</td>\n",
       "      <td>Even if semis and the market continue rallying...</td>\n",
       "      <td>186</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40782</th>\n",
       "      <td>14hs93o</td>\n",
       "      <td>Jensen Huang finally sold some NVDA stock.</td>\n",
       "      <td>2023-06-24</td>\n",
       "      <td>Better link to show insider trading\\n\\n[http:/...</td>\n",
       "      <td>521</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40785</th>\n",
       "      <td>14hs93o</td>\n",
       "      <td>Jensen Huang finally sold some NVDA stock.</td>\n",
       "      <td>2023-06-24</td>\n",
       "      <td>We will see how much MM would push NVDA higher.</td>\n",
       "      <td>47</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37800</th>\n",
       "      <td>14hnddl</td>\n",
       "      <td>That was fast. Indian PM gets wish granted in ...</td>\n",
       "      <td>2023-06-24</td>\n",
       "      <td>Not necessary.  Amazon is basically Indian.  M...</td>\n",
       "      <td>182</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Post ID                                              Title        Date  \\\n",
       "19923  14hw652         LOST 11k in four days and blew my account.  2023-06-24   \n",
       "40781  14hs93o         Jensen Huang finally sold some NVDA stock.  2023-06-24   \n",
       "40782  14hs93o         Jensen Huang finally sold some NVDA stock.  2023-06-24   \n",
       "40785  14hs93o         Jensen Huang finally sold some NVDA stock.  2023-06-24   \n",
       "37800  14hnddl  That was fast. Indian PM gets wish granted in ...  2023-06-24   \n",
       "\n",
       "                                                 Comment  Length  \\\n",
       "19923  All ya had to do was put it in Tesla and Amazo...      62   \n",
       "40781  Even if semis and the market continue rallying...     186   \n",
       "40782  Better link to show insider trading\\n\\n[http:/...     521   \n",
       "40785    We will see how much MM would push NVDA higher.      47   \n",
       "37800  Not necessary.  Amazon is basically Indian.  M...     182   \n",
       "\n",
       "             Keyword  Sentiment  \n",
       "19923  Tesla Amazon           1  \n",
       "40781        Nvidia           1  \n",
       "40782        Nvidia           1  \n",
       "40785        Nvidia           1  \n",
       "37800        Amazon           1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = dataframeProcessor(comments, classifier=classifier)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'06-24': 9, '06-25': 18, '06-26': 37, '06-27': 32, '06-28': 61, '06-29': 23, '06-30': 41, '07-01': 26, '07-02': 75, '07-03': 134, '07-04': 35, '07-05': 56, '07-06': 82, '07-07': 43, '07-08': 8, '07-09': 36, '07-10': 79, '07-11': 47, '07-12': 34, '07-13': 32, '07-14': 98, '07-15': 39, '07-16': 80, '07-17': 133, '07-18': 70, '07-19': 209, '07-20': 161, '07-21': 90, '07-22': 14, '07-23': 27, '07-24': 0}\n",
      "{'06-24': 0, '06-25': 9, '06-26': 18, '06-27': 18, '06-28': 34, '06-29': 32, '06-30': 110, '07-01': 33, '07-02': 27, '07-03': 40, '07-04': 21, '07-05': 27, '07-06': 26, '07-07': 12, '07-08': 10, '07-09': 7, '07-10': 24, '07-11': 6, '07-12': 10, '07-13': 20, '07-14': 18, '07-15': 12, '07-16': 11, '07-17': 19, '07-18': 8, '07-19': 49, '07-20': 12, '07-21': 12, '07-22': 10, '07-23': 23, '07-24': 4}\n",
      "{'06-24': 8, '06-25': 10, '06-26': 36, '06-27': 75, '06-28': 66, '06-29': 16, '06-30': 27, '07-01': 23, '07-02': 40, '07-03': 17, '07-04': 26, '07-05': 17, '07-06': 6, '07-07': 19, '07-08': 4, '07-09': 3, '07-10': 13, '07-11': 9, '07-12': 21, '07-13': 107, '07-14': 125, '07-15': 27, '07-16': 32, '07-17': 69, '07-18': 55, '07-19': 26, '07-20': 20, '07-21': 35, '07-22': 47, '07-23': 21, '07-24': 0}\n",
      "{'06-24': 2, '06-25': 4, '06-26': 2, '06-27': 7, '06-28': 11, '06-29': 0, '06-30': 5, '07-01': 11, '07-02': 1, '07-03': 7, '07-04': 1, '07-05': 4, '07-06': 3, '07-07': 5, '07-08': 2, '07-09': 4, '07-10': 13, '07-11': 5, '07-12': 2, '07-13': 16, '07-14': 12, '07-15': 3, '07-16': 7, '07-17': 5, '07-18': 6, '07-19': 3, '07-20': 4, '07-21': 19, '07-22': 1, '07-23': 7, '07-24': 1}\n",
      "{'06-24': 5, '06-25': 1, '06-26': 1, '06-27': 11, '06-28': 10, '06-29': 9, '06-30': 6, '07-01': 4, '07-02': 2, '07-03': 6, '07-04': 6, '07-05': 4, '07-06': 5, '07-07': 5, '07-08': 10, '07-09': 4, '07-10': 14, '07-11': 15, '07-12': 8, '07-13': 13, '07-14': 7, '07-15': 9, '07-16': 8, '07-17': 14, '07-18': 12, '07-19': 2, '07-20': 3, '07-21': 20, '07-22': 3, '07-23': 6, '07-24': 2}\n",
      "{'06-24': 2, '06-25': 2, '06-26': 3, '06-27': 1, '06-28': 6, '06-29': 1, '06-30': 4, '07-01': 4, '07-02': 1, '07-03': 1, '07-04': 2, '07-05': 5, '07-06': 3, '07-07': 2, '07-08': 6, '07-09': 2, '07-10': 3, '07-11': 6, '07-12': 5, '07-13': 5, '07-14': 4, '07-15': 3, '07-16': 5, '07-17': 5, '07-18': 35, '07-19': 6, '07-20': 5, '07-21': 14, '07-22': 2, '07-23': 5, '07-24': 0}\n",
      "{'06-24': 0, '06-25': 1, '06-26': 2, '06-27': 6, '06-28': 8, '06-29': 1, '06-30': 7, '07-01': 5, '07-02': 1, '07-03': 8, '07-04': 6, '07-05': 82, '07-06': 62, '07-07': 43, '07-08': 13, '07-09': 33, '07-10': 55, '07-11': 7, '07-12': 28, '07-13': 11, '07-14': 9, '07-15': 4, '07-16': 8, '07-17': 6, '07-18': 4, '07-19': 6, '07-20': 5, '07-21': 28, '07-22': 4, '07-23': 10, '07-24': 18}\n"
     ]
    }
   ],
   "source": [
    "jsonList = jsonBuilder(processed_df)\n",
    "\n",
    "processed_df.to_csv('/Users/aadeesh/redditSentiment/server/Data/redditData/Posts/processed_df.csv')\n",
    "for i in jsonList:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
