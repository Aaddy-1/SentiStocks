{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/aadeesh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EasyDataAugmenter\n",
    "\n",
    "import random\n",
    "\n",
    "augmenter = EasyDataAugmenter()\n",
    "\n",
    "def augment_text(sentence):\n",
    "    augmented_sentences = augmenter.augment(sentence)\n",
    "    if augmented_sentences:\n",
    "        return random.choice(augmented_sentences)\n",
    "    else:\n",
    "        return sentence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One reviewer mentioned watching 1 oz episode\n"
     ]
    }
   ],
   "source": [
    "# Cleans the sentence of links and emojis, removes stop words, and lemmatizes words\n",
    "def commentCleaner(comment):\n",
    "    # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "    comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "    \n",
    "    # Tokenize the comment\n",
    "    tokens = comment.split()\n",
    "    # tokens = comment.split(' ')\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_comment = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_comment\n",
    "\n",
    "print(commentCleaner(\"One of the other reviewers mentioned watching 1 oz episode\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDf(df):\n",
    "    df[\"Sentence\"] = df[\"Sentence\"].apply(commentCleaner)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentDataFrame(df):\n",
    "    augmented_data = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for sentence, sentiment in zip(df.Sentence, df.Sentiment):\n",
    "        random_num = random.randint(1, 2)\n",
    "        if (random_num == 1):\n",
    "            augmented_sentence = augment_text(sentence)\n",
    "            augmented_data.append(augmented_sentence)\n",
    "            augmented_labels.append(sentiment)\n",
    "    \n",
    "    new_df = {\"Sentence\" : augmented_data, \"Sentiment\" : augmented_labels}\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "\n",
    "    df = pd.concat([df, new_df])\n",
    "    # df = df.append(pd.DataFrame(new_df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5842, 2)\n",
      "                                            Sentence Sentiment\n",
      "0  The GeoSolutions technology will leverage Bene...  positive\n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
      "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
      "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
      "4  The Swedish buyout firm has sold its remaining...   neutral\n",
      "(8844, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Augmenting the data\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "data = augmentDataFrame(data)\n",
    "print(data.shape)\n",
    "data.to_csv('augmented_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8844, 2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning our data\n",
    "cleaned_data = cleanDf(data)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8844, 2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giving integer labels to our sentiment labels\n",
    "lb = LabelEncoder()\n",
    "cleaned_data[\"Sentiment\"] = lb.fit_transform(data['Sentiment'])\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentences\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(cleaned_data['Sentence'].values)\n",
    "X = tokenizer.texts_to_sequences(cleaned_data['Sentence'].values)\n",
    "X = pad_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 28, 120)           60000     \n",
      "                                                                 \n",
      " spatial_dropout1d_6 (Spati  (None, 28, 120)           0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 200)               256800    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 120)               12120     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 349383 (1.33 MB)\n",
      "Trainable params: 349383 (1.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Building our model\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"trial1/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# Create a ModelCheckpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback to stop training if validation loss doesn't improve\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,  # Number of epochs with no improvement after which training will stop\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the integer labels to onehot encoding\n",
    "y=pd.get_dummies(data['Sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.8948\n",
      "Epoch 1: loss improved from 0.19203 to 0.19007, saving model to trial1/weights-improvement-01-0.1901.hdf5\n",
      "194/194 [==============================] - 17s 86ms/step - loss: 0.1901 - accuracy: 0.8948\n",
      "Epoch 2/80\n",
      "  2/194 [..............................] - ETA: 15s - loss: 0.1131 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadeesh/miniconda3/envs/redditEnv/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.8981\n",
      "Epoch 2: loss improved from 0.19007 to 0.18004, saving model to trial1/weights-improvement-02-0.1800.hdf5\n",
      "194/194 [==============================] - 16s 82ms/step - loss: 0.1800 - accuracy: 0.8981\n",
      "Epoch 3/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.8950\n",
      "Epoch 3: loss did not improve from 0.18004\n",
      "194/194 [==============================] - 17s 87ms/step - loss: 0.1971 - accuracy: 0.8950\n",
      "Epoch 4/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.8976\n",
      "Epoch 4: loss did not improve from 0.18004\n",
      "194/194 [==============================] - 16s 81ms/step - loss: 0.1895 - accuracy: 0.8976\n",
      "Epoch 5/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.8976\n",
      "Epoch 5: loss did not improve from 0.18004\n",
      "194/194 [==============================] - 16s 84ms/step - loss: 0.1851 - accuracy: 0.8976\n",
      "Epoch 6/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.8984\n",
      "Epoch 6: loss did not improve from 0.18004\n",
      "194/194 [==============================] - 16s 81ms/step - loss: 0.1927 - accuracy: 0.8984\n",
      "Epoch 7/80\n",
      "194/194 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.8995\n",
      "Epoch 7: loss did not improve from 0.18004\n",
      "194/194 [==============================] - 16s 81ms/step - loss: 0.1915 - accuracy: 0.8995\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2dae70370>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 80, \n",
    "          batch_size=batch_size, \n",
    "          verbose = 'auto', \n",
    "          callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
