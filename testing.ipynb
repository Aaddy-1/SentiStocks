{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14mo2g0</td>\n",
       "      <td>https://i.redd.it/xwyomcqbi29b1.jpg</td>\n",
       "      <td>The Future of investment expertise</td>\n",
       "      <td>650</td>\n",
       "      <td>32219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14llabn</td>\n",
       "      <td>https://v.redd.it/w6s5ehaxqt8b1</td>\n",
       "      <td>Musk vs Zuckerberg. The fight of the Century !!!</td>\n",
       "      <td>496</td>\n",
       "      <td>12661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>https://www.cnbc.com/2023/06/30/supreme-court-...</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>3024</td>\n",
       "      <td>10994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14l8j3k</td>\n",
       "      <td>https://i.redd.it/qcf3y93g8r8b1.png</td>\n",
       "      <td>Home prices in the US declined for the first t...</td>\n",
       "      <td>1166</td>\n",
       "      <td>10699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14lx2do</td>\n",
       "      <td>https://i.redd.it/l1vb7439gw8b1.jpg</td>\n",
       "      <td>My therapist tells me to join this sub. Why?</td>\n",
       "      <td>312</td>\n",
       "      <td>8427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14q3zm1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>The Bull Market is Just Starting</td>\n",
       "      <td>349</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14pt4eq</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Group Barbeque Thread for July 4th, 2023</td>\n",
       "      <td>5798</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14lngwo</td>\n",
       "      <td>https://teamster.org/2023/06/teamsters-nationw...</td>\n",
       "      <td>Teamsters: Nationwide UPS Strike is Imminent</td>\n",
       "      <td>59</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14pjr3j</td>\n",
       "      <td>https://www.reuters.com/technology/amds-ai-chi...</td>\n",
       "      <td>AMD AI chips could match Nvidia offerings, sof...</td>\n",
       "      <td>69</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14nbryb</td>\n",
       "      <td>https://www.reddit.com/gallery/14nbryb</td>\n",
       "      <td>Tim Apple $AAPL 40k YOLOüçéüçè</td>\n",
       "      <td>53</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                                url  \\\n",
       "0   14mo2g0                https://i.redd.it/xwyomcqbi29b1.jpg   \n",
       "1   14llabn                    https://v.redd.it/w6s5ehaxqt8b1   \n",
       "2   14n378b  https://www.cnbc.com/2023/06/30/supreme-court-...   \n",
       "3   14l8j3k                https://i.redd.it/qcf3y93g8r8b1.png   \n",
       "4   14lx2do                https://i.redd.it/l1vb7439gw8b1.jpg   \n",
       "..      ...                                                ...   \n",
       "95  14q3zm1  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "96  14pt4eq  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "97  14lngwo  https://teamster.org/2023/06/teamsters-nationw...   \n",
       "98  14pjr3j  https://www.reuters.com/technology/amds-ai-chi...   \n",
       "99  14nbryb             https://www.reddit.com/gallery/14nbryb   \n",
       "\n",
       "                                                Title  Total Comments  Score  \n",
       "0                  The Future of investment expertise             650  32219  \n",
       "1    Musk vs Zuckerberg. The fight of the Century !!!             496  12661  \n",
       "2   Supreme Court strikes down student loan forgiv...            3024  10994  \n",
       "3   Home prices in the US declined for the first t...            1166  10699  \n",
       "4        My therapist tells me to join this sub. Why?             312   8427  \n",
       "..                                                ...             ...    ...  \n",
       "95                   The Bull Market is Just Starting             349    199  \n",
       "96           Group Barbeque Thread for July 4th, 2023            5798    173  \n",
       "97       Teamsters: Nationwide UPS Strike is Imminent              59    173  \n",
       "98  AMD AI chips could match Nvidia offerings, sof...              69    174  \n",
       "99                         Tim Apple $AAPL 40k YOLOüçéüçè              53    155  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"Top_This_Week.csv\")\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Num_Comments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14mo2g0</td>\n",
       "      <td>2023-06-30 02:38:33</td>\n",
       "      <td>650</td>\n",
       "      <td>[https://preview.redd.it/du20cvgfz29b1.jpeg?wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14llabn</td>\n",
       "      <td>2023-06-28 21:11:26</td>\n",
       "      <td>496</td>\n",
       "      <td>[Danm Charlie Sheen was jacked, Nvidia working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>3030</td>\n",
       "      <td>[That means fewer day traders?, Should've take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14l8j3k</td>\n",
       "      <td>2023-06-28 12:44:00</td>\n",
       "      <td>1166</td>\n",
       "      <td>[wow awesome. Prices went from \"unaffordable\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14lx2do</td>\n",
       "      <td>2023-06-29 06:16:13</td>\n",
       "      <td>310</td>\n",
       "      <td>[I‚Äôm in this picture and I don‚Äôt like it., Sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14q3zm1</td>\n",
       "      <td>2023-07-04 03:50:09</td>\n",
       "      <td>442</td>\n",
       "      <td>[Time to buy puts on everything, The top is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14pt4eq</td>\n",
       "      <td>2023-07-03 20:00:34</td>\n",
       "      <td>5822</td>\n",
       "      <td>[I swear to God I just watched a video where a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14lngwo</td>\n",
       "      <td>2023-06-28 22:38:19</td>\n",
       "      <td>59</td>\n",
       "      <td>[Im thinking puts bois, Bullish\\n\\nhttps://pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14pjr3j</td>\n",
       "      <td>2023-07-03 13:56:32</td>\n",
       "      <td>72</td>\n",
       "      <td>[Real problem for AMD isn‚Äôt performance, it‚Äôs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14nbryb</td>\n",
       "      <td>2023-06-30 20:52:09</td>\n",
       "      <td>53</td>\n",
       "      <td>[It was a $8500 yolo that went to $40k. Congra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                Date  Num_Comments  \\\n",
       "0   14mo2g0 2023-06-30 02:38:33           650   \n",
       "1   14llabn 2023-06-28 21:11:26           496   \n",
       "2   14n378b 2023-06-30 15:16:01          3030   \n",
       "3   14l8j3k 2023-06-28 12:44:00          1166   \n",
       "4   14lx2do 2023-06-29 06:16:13           310   \n",
       "..      ...                 ...           ...   \n",
       "95  14q3zm1 2023-07-04 03:50:09           442   \n",
       "96  14pt4eq 2023-07-03 20:00:34          5822   \n",
       "97  14lngwo 2023-06-28 22:38:19            59   \n",
       "98  14pjr3j 2023-07-03 13:56:32            72   \n",
       "99  14nbryb 2023-06-30 20:52:09            53   \n",
       "\n",
       "                                             Comments  \n",
       "0   [https://preview.redd.it/du20cvgfz29b1.jpeg?wi...  \n",
       "1   [Danm Charlie Sheen was jacked, Nvidia working...  \n",
       "2   [That means fewer day traders?, Should've take...  \n",
       "3   [wow awesome. Prices went from \"unaffordable\" ...  \n",
       "4   [I‚Äôm in this picture and I don‚Äôt like it., Sum...  \n",
       "..                                                ...  \n",
       "95  [Time to buy puts on everything, The top is in...  \n",
       "96  [I swear to God I just watched a video where a...  \n",
       "97  [Im thinking puts bois, Bullish\\n\\nhttps://pre...  \n",
       "98  [Real problem for AMD isn‚Äôt performance, it‚Äôs ...  \n",
       "99  [It was a $8500 yolo that went to $40k. Congra...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# loading env file\n",
    "load_dotenv('environment.env')\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id = os.getenv('CLIENT_ID'),\n",
    "                    client_secret = os.getenv('CLIENT_SECRET'),\n",
    "                    user_agent = os.getenv('USER_AGENT'))\n",
    "\n",
    "\n",
    "\n",
    "ids = my_data[\"ID\"]\n",
    "comments = {\"Post ID\" : [], \"Date\" : [], \"Num_Comments\" : [], \"Comments\" : []}\n",
    "\n",
    "for id in ids:\n",
    "    submission = reddit.submission(id = id)\n",
    "    comments[\"Post ID\"].append(id)\n",
    "    comments[\"Date\"].append(datetime.datetime.utcfromtimestamp(submission.created_utc))\n",
    "    comments[\"Num_Comments\"].append(submission.num_comments)\n",
    "    comments_array = []\n",
    "    for commentInstance in submission.comments[1:]:\n",
    "        if isinstance(commentInstance, MoreComments):\n",
    "            continue\n",
    "        comments_array.append(commentInstance.body)\n",
    "    comments[\"Comments\"].append(comments_array)\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "comments_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39msize)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(comments_df.size)\n",
    "print(comments_df.shape)\n",
    "print(comments_df.head())\n",
    "\n",
    "comments_df.to_csv(\"Comments_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>That means fewer day traders?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>Should've taken out a PPP loan and paid ya stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>I don't get why they don't just set student lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>Doordash in shambles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>There‚Äôs gonna be so many used dodge chargers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post ID                                              Title  \\\n",
       "0  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "1  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "2  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "3  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "4  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "\n",
       "                  Date                                            Comment  \n",
       "0  2023-06-30 15:16:01                      That means fewer day traders?  \n",
       "1  2023-06-30 15:16:01  Should've taken out a PPP loan and paid ya stu...  \n",
       "2  2023-06-30 15:16:01  I don't get why they don't just set student lo...  \n",
       "3  2023-06-30 15:16:01                               Doordash in shambles  \n",
       "4  2023-06-30 15:16:01       There‚Äôs gonna be so many used dodge chargers  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning comments_df\n",
    "\n",
    "new_df = pd.read_csv(\"Comments.csv\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aadeesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aadeesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample comment It contains some stop words like the is and a emojis are  great job \n",
      "['This', 'is', 'a', 'sample', 'comment', 'It', 'contains', 'some', 'stop', 'words', 'like', 'the', 'is', 'and', 'a', 'emojis', 'are', 'great', 'job']\n",
      "This sample comment It contains stop words like emojis great job\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(10):\n",
    "#     print(new_df[\"Comment\"][i])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def commentCleaner(comment):\n",
    "    # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "    comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "    \n",
    "    print(comment)\n",
    "    # Tokenize the comment\n",
    "    tokens = word_tokenize(comment)\n",
    "    # tokens = comment.split(' ')\n",
    "    print(tokens)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove URLs\n",
    "    tokens = [token for token in tokens if not token.startswith(\"http\")]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_comment = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_comment\n",
    "print(commentCleaner(\"This is a sample comment. It contains some stop words like the, is, and a. emojis are üöÄ, great job u/Demonic_Mercenary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[  101  2026  2171  2003  9779 26095  9953 14654  2026  2171  2003  9779\n",
      "  26095  9953 14654  2026  2171  2003  9779 26095  9953 14654  2026  2171\n",
      "   2003  9779 26095  9953 14654  2026  2171  2003  9779 26095  9953 14654\n",
      "    102     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [  101  2017  2024  1037  2200  5281  3698  4083  6739  2040  2064  9611\n",
      "   2151  3160  1012  2129  2052  1045  4339  1037  2565  2000 19204  4697\n",
      "   3793  2000  2191  2009  7218  2005 14324 15792  4106  1029   102     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]], shape=(2, 64), dtype=int32)\n",
      "\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(2, 64), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def tokenize_comment(comment, max_length):\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize the comment\n",
    "    tokenized_text = tokenizer.batch_encode_plus(\n",
    "        comment,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Extract the tokenized input IDs and attention mask\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_mask = tokenized_text['attention_mask']\n",
    "    # input_ids = tf.convert_to_tensor(input_ids)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "text = [\"My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma\", \n",
    "        \"You are a very experienced machine learning expert who can solve any question. How would I write a program to tokenize text to make it suitable for bert sentiment analysis?\"]\n",
    "max_length = 64\n",
    "input_ids, attention_mask = tokenize_comment(text, max_length) \n",
    "print(type(input_ids))\n",
    "print(input_ids)\n",
    "print()\n",
    "print(type(attention_mask))\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"$TSLA\", \"TSLA\", \"Tesla\", \"tesla\", \n",
    "            \"$AAPL\", \"AAPL\", \"aapl\", \"Apple\", \"apple\", \n",
    "            \"$NVDA\", \"NVDA\", \"Nvidia\", \"nvidia\", \n",
    "            \"$GOOGL\", \"GOOGL\", \"Google\", \"google\", \"Alphabet\", \"alphabet\",\n",
    "            \"$AMZN\", \"AMZN\", \"Amazon\", \"amazon\"\n",
    "            \"$MSFT\", \"MSFT\", \"Microsoft\", \"microsoft\"\n",
    "            \"$META\", \"META\", \"Meta\", \"meta\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv(\"PastWeekComments.csv\")\n",
    "len(comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7766, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering our dataframe to get the comments which contain our required keywords\n",
    "mask = comments['Title'].str.contains('|'.join(keywords), case=False) | \\\n",
    "    comments['Comment'].str.contains('|'.join(keywords), case=False)\n",
    "\n",
    "# Apply the mask to filter the dataframe\n",
    "filtered_df = comments[mask]\n",
    "\n",
    "filtered_df.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
