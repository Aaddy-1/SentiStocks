{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14mo2g0</td>\n",
       "      <td>https://i.redd.it/xwyomcqbi29b1.jpg</td>\n",
       "      <td>The Future of investment expertise</td>\n",
       "      <td>650</td>\n",
       "      <td>32219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14llabn</td>\n",
       "      <td>https://v.redd.it/w6s5ehaxqt8b1</td>\n",
       "      <td>Musk vs Zuckerberg. The fight of the Century !!!</td>\n",
       "      <td>496</td>\n",
       "      <td>12661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>https://www.cnbc.com/2023/06/30/supreme-court-...</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>3024</td>\n",
       "      <td>10994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14l8j3k</td>\n",
       "      <td>https://i.redd.it/qcf3y93g8r8b1.png</td>\n",
       "      <td>Home prices in the US declined for the first t...</td>\n",
       "      <td>1166</td>\n",
       "      <td>10699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14lx2do</td>\n",
       "      <td>https://i.redd.it/l1vb7439gw8b1.jpg</td>\n",
       "      <td>My therapist tells me to join this sub. Why?</td>\n",
       "      <td>312</td>\n",
       "      <td>8427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14q3zm1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>The Bull Market is Just Starting</td>\n",
       "      <td>349</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14pt4eq</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Group Barbeque Thread for July 4th, 2023</td>\n",
       "      <td>5798</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14lngwo</td>\n",
       "      <td>https://teamster.org/2023/06/teamsters-nationw...</td>\n",
       "      <td>Teamsters: Nationwide UPS Strike is Imminent</td>\n",
       "      <td>59</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14pjr3j</td>\n",
       "      <td>https://www.reuters.com/technology/amds-ai-chi...</td>\n",
       "      <td>AMD AI chips could match Nvidia offerings, sof...</td>\n",
       "      <td>69</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14nbryb</td>\n",
       "      <td>https://www.reddit.com/gallery/14nbryb</td>\n",
       "      <td>Tim Apple $AAPL 40k YOLOüçéüçè</td>\n",
       "      <td>53</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                                url  \\\n",
       "0   14mo2g0                https://i.redd.it/xwyomcqbi29b1.jpg   \n",
       "1   14llabn                    https://v.redd.it/w6s5ehaxqt8b1   \n",
       "2   14n378b  https://www.cnbc.com/2023/06/30/supreme-court-...   \n",
       "3   14l8j3k                https://i.redd.it/qcf3y93g8r8b1.png   \n",
       "4   14lx2do                https://i.redd.it/l1vb7439gw8b1.jpg   \n",
       "..      ...                                                ...   \n",
       "95  14q3zm1  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "96  14pt4eq  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "97  14lngwo  https://teamster.org/2023/06/teamsters-nationw...   \n",
       "98  14pjr3j  https://www.reuters.com/technology/amds-ai-chi...   \n",
       "99  14nbryb             https://www.reddit.com/gallery/14nbryb   \n",
       "\n",
       "                                                Title  Total Comments  Score  \n",
       "0                  The Future of investment expertise             650  32219  \n",
       "1    Musk vs Zuckerberg. The fight of the Century !!!             496  12661  \n",
       "2   Supreme Court strikes down student loan forgiv...            3024  10994  \n",
       "3   Home prices in the US declined for the first t...            1166  10699  \n",
       "4        My therapist tells me to join this sub. Why?             312   8427  \n",
       "..                                                ...             ...    ...  \n",
       "95                   The Bull Market is Just Starting             349    199  \n",
       "96           Group Barbeque Thread for July 4th, 2023            5798    173  \n",
       "97       Teamsters: Nationwide UPS Strike is Imminent              59    173  \n",
       "98  AMD AI chips could match Nvidia offerings, sof...              69    174  \n",
       "99                         Tim Apple $AAPL 40k YOLOüçéüçè              53    155  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"Top_This_Week.csv\")\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Num_Comments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14mo2g0</td>\n",
       "      <td>2023-06-30 02:38:33</td>\n",
       "      <td>650</td>\n",
       "      <td>[https://preview.redd.it/du20cvgfz29b1.jpeg?wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14llabn</td>\n",
       "      <td>2023-06-28 21:11:26</td>\n",
       "      <td>496</td>\n",
       "      <td>[Danm Charlie Sheen was jacked, Nvidia working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>3030</td>\n",
       "      <td>[That means fewer day traders?, Should've take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14l8j3k</td>\n",
       "      <td>2023-06-28 12:44:00</td>\n",
       "      <td>1166</td>\n",
       "      <td>[wow awesome. Prices went from \"unaffordable\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14lx2do</td>\n",
       "      <td>2023-06-29 06:16:13</td>\n",
       "      <td>310</td>\n",
       "      <td>[I‚Äôm in this picture and I don‚Äôt like it., Sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14q3zm1</td>\n",
       "      <td>2023-07-04 03:50:09</td>\n",
       "      <td>442</td>\n",
       "      <td>[Time to buy puts on everything, The top is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14pt4eq</td>\n",
       "      <td>2023-07-03 20:00:34</td>\n",
       "      <td>5822</td>\n",
       "      <td>[I swear to God I just watched a video where a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14lngwo</td>\n",
       "      <td>2023-06-28 22:38:19</td>\n",
       "      <td>59</td>\n",
       "      <td>[Im thinking puts bois, Bullish\\n\\nhttps://pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14pjr3j</td>\n",
       "      <td>2023-07-03 13:56:32</td>\n",
       "      <td>72</td>\n",
       "      <td>[Real problem for AMD isn‚Äôt performance, it‚Äôs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14nbryb</td>\n",
       "      <td>2023-06-30 20:52:09</td>\n",
       "      <td>53</td>\n",
       "      <td>[It was a $8500 yolo that went to $40k. Congra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                Date  Num_Comments  \\\n",
       "0   14mo2g0 2023-06-30 02:38:33           650   \n",
       "1   14llabn 2023-06-28 21:11:26           496   \n",
       "2   14n378b 2023-06-30 15:16:01          3030   \n",
       "3   14l8j3k 2023-06-28 12:44:00          1166   \n",
       "4   14lx2do 2023-06-29 06:16:13           310   \n",
       "..      ...                 ...           ...   \n",
       "95  14q3zm1 2023-07-04 03:50:09           442   \n",
       "96  14pt4eq 2023-07-03 20:00:34          5822   \n",
       "97  14lngwo 2023-06-28 22:38:19            59   \n",
       "98  14pjr3j 2023-07-03 13:56:32            72   \n",
       "99  14nbryb 2023-06-30 20:52:09            53   \n",
       "\n",
       "                                             Comments  \n",
       "0   [https://preview.redd.it/du20cvgfz29b1.jpeg?wi...  \n",
       "1   [Danm Charlie Sheen was jacked, Nvidia working...  \n",
       "2   [That means fewer day traders?, Should've take...  \n",
       "3   [wow awesome. Prices went from \"unaffordable\" ...  \n",
       "4   [I‚Äôm in this picture and I don‚Äôt like it., Sum...  \n",
       "..                                                ...  \n",
       "95  [Time to buy puts on everything, The top is in...  \n",
       "96  [I swear to God I just watched a video where a...  \n",
       "97  [Im thinking puts bois, Bullish\\n\\nhttps://pre...  \n",
       "98  [Real problem for AMD isn‚Äôt performance, it‚Äôs ...  \n",
       "99  [It was a $8500 yolo that went to $40k. Congra...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# loading env file\n",
    "load_dotenv('environment.env')\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id = os.getenv('CLIENT_ID'),\n",
    "                    client_secret = os.getenv('CLIENT_SECRET'),\n",
    "                    user_agent = os.getenv('USER_AGENT'))\n",
    "\n",
    "\n",
    "\n",
    "ids = my_data[\"ID\"]\n",
    "comments = {\"Post ID\" : [], \"Date\" : [], \"Num_Comments\" : [], \"Comments\" : []}\n",
    "\n",
    "for id in ids:\n",
    "    submission = reddit.submission(id = id)\n",
    "    comments[\"Post ID\"].append(id)\n",
    "    comments[\"Date\"].append(datetime.datetime.utcfromtimestamp(submission.created_utc))\n",
    "    comments[\"Num_Comments\"].append(submission.num_comments)\n",
    "    comments_array = []\n",
    "    for commentInstance in submission.comments[1:]:\n",
    "        if isinstance(commentInstance, MoreComments):\n",
    "            continue\n",
    "        comments_array.append(commentInstance.body)\n",
    "    comments[\"Comments\"].append(comments_array)\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "comments_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39msize)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(comments_df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(comments_df.size)\n",
    "print(comments_df.shape)\n",
    "print(comments_df.head())\n",
    "\n",
    "comments_df.to_csv(\"Comments_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>That means fewer day traders?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>Should've taken out a PPP loan and paid ya stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>I don't get why they don't just set student lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>Doordash in shambles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>2023-06-30 15:16:01</td>\n",
       "      <td>There‚Äôs gonna be so many used dodge chargers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post ID                                              Title  \\\n",
       "0  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "1  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "2  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "3  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "4  14n378b  Supreme Court strikes down student loan forgiv...   \n",
       "\n",
       "                  Date                                            Comment  \n",
       "0  2023-06-30 15:16:01                      That means fewer day traders?  \n",
       "1  2023-06-30 15:16:01  Should've taken out a PPP loan and paid ya stu...  \n",
       "2  2023-06-30 15:16:01  I don't get why they don't just set student lo...  \n",
       "3  2023-06-30 15:16:01                               Doordash in shambles  \n",
       "4  2023-06-30 15:16:01       There‚Äôs gonna be so many used dodge chargers  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning comments_df\n",
    "\n",
    "new_df = pd.read_csv(\"Comments.csv\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aadeesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aadeesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample comment It contains stop words like emojis great job\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(10):\n",
    "#     print(new_df[\"Comment\"][i])\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def commentCleaner(comment):\n",
    "    # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "    comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "    \n",
    "    # Tokenize the comment\n",
    "    tokens = word_tokenize(comment)\n",
    "    # tokens = comment.split(' ')\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove URLs\n",
    "    tokens = [token for token in tokens if not token.startswith(\"http\")]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_comment = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_comment\n",
    "print(commentCleaner(\"This is a sample comment. It contains some stop words like the, is, and a. emojis are üöÄ, great job u/Demonic_Mercenary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[  101,  2026,  2171,  2003,  9779, 26095,  9953, 14654,  2026,  2171,\n",
      "          2003,  9779, 26095,  9953, 14654,  2026,  2171,  2003,  9779, 26095,\n",
      "          9953, 14654,  2026,  2171,  2003,  9779, 26095,  9953, 14654,  2026,\n",
      "          2171,  2003,  9779, 26095,  9953, 14654,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2017,  2024,  1037,  2200,  5281,  3698,  4083,  6739,  2040,\n",
      "          2064,  9611,  2151,  3160,  1012,  2129,  2052,  1045,  4339,  1037,\n",
      "          2565,  2000, 19204,  4697,  3793,  2000,  2191,  2009,  7218,  2005,\n",
      "         14324, 15792,  4106,  1029,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def tokenize_comment(comment, max_length):\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize the comment\n",
    "    tokenized_text = tokenizer.batch_encode_plus(\n",
    "        comment,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Extract the tokenized input IDs and attention mask\n",
    "    input_ids = tokenized_text['input_ids']\n",
    "    attention_mask = tokenized_text['attention_mask']\n",
    "    # input_ids = tf.convert_to_tensor(input_ids)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "text = [\"My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma My name is Aadeeesh Sharma\", \n",
    "        \"You are a very experienced machine learning expert who can solve any question. How would I write a program to tokenize text to make it suitable for bert sentiment analysis?\"]\n",
    "max_length = 64\n",
    "input_ids, attention_mask = tokenize_comment(text, max_length) \n",
    "print(type(input_ids))\n",
    "print(input_ids)\n",
    "print()\n",
    "print(type(attention_mask))\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"$TSLA\", \"TSLA\", \"Tesla\", \"tesla\", \n",
    "            \"$AAPL\", \"AAPL\", \"aapl\", \"Apple\", \"apple\", \n",
    "            \"$NVDA\", \"NVDA\", \"Nvidia\", \"nvidia\", \n",
    "            \"$GOOGL\", \"GOOGL\", \"Google\", \"google\", \"Alphabet\", \"alphabet\",\n",
    "            \"$AMZN\", \"AMZN\", \"Amazon\", \"amazon\"\n",
    "            \"$MSFT\", \"MSFT\", \"Microsoft\", \"microsoft\"\n",
    "            \"$META\", \"META\", \"Meta\", \"meta\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv(\"PastWeekComments.csv\")\n",
    "len(comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I somehow don't think that this is what any Te...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>They‚Äôre going to fake fight in the metaverse</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I can't wait for Tesla fan boys to lose all th...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I mean I have meta calls right now so i'll go ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I'm all in on Meta Knight.</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                  Title                 Date  \\\n",
       "6   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "16  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "26  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "30  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "38  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "\n",
       "                                              Comment  Length  \n",
       "6   I somehow don't think that this is what any Te...     134  \n",
       "16       They‚Äôre going to fake fight in the metaverse      44  \n",
       "26  I can't wait for Tesla fan boys to lose all th...      76  \n",
       "30  I mean I have meta calls right now so i'll go ...      59  \n",
       "38                         I'm all in on Meta Knight.      26  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering our dataframe to get the comments which contain our required keywords\n",
    "mask = comments['Title'].str.contains('|'.join(keywords), case=False) | \\\n",
    "    comments['Comment'].str.contains('|'.join(keywords), case=False)\n",
    "\n",
    "# Apply the mask to filter the dataframe\n",
    "filtered_df = comments[mask]\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7766.000000\n",
       "mean      105.005279\n",
       "std       200.085983\n",
       "min         1.000000\n",
       "25%        31.000000\n",
       "50%        60.000000\n",
       "75%       115.000000\n",
       "max      6213.000000\n",
       "Name: Length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[\"Length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I somehow don't think that this is what any Te...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>They‚Äôre going to fake fight in the metaverse</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I can't wait for Tesla fan boys to lose all th...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I mean I have meta calls right now so i'll go ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I'm all in on Meta Knight.</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                  Title                 Date  \\\n",
       "6   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "16  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "26  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "30  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "38  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "\n",
       "                                              Comment  Length  \n",
       "6   I somehow don't think that this is what any Te...     134  \n",
       "16       They‚Äôre going to fake fight in the metaverse      44  \n",
       "26  I can't wait for Tesla fan boys to lose all th...      76  \n",
       "30  I mean I have meta calls right now so i'll go ...      59  \n",
       "38                         I'm all in on Meta Knight.      26  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean I meta calls right ill go zukk\n"
     ]
    }
   ],
   "source": [
    "copied_df = filtered_df.copy()\n",
    "\n",
    "copied_df[\"Comment\"] = filtered_df[\"Comment\"].map(commentCleaner)\n",
    "\n",
    "copied_df.head()\n",
    "\n",
    "# filtered_df[\"Comment\"] = filtered_df[\"Comment\"].map(commentCleaner)\n",
    "\n",
    "# filtered_df.head()\n",
    "print(copied_df[\"Comment\"][30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Post ID                  Title                 Date  \\\n",
      "6   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
      "16  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
      "26  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
      "30  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
      "38  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
      "\n",
      "                                              Comment  Length  \n",
      "6   I somehow dont think Tesla investors mind hear...     134  \n",
      "16                  Theyre going fake fight metaverse      44  \n",
      "26  I cant wait Tesla fan boys lose fucking money ...      76  \n",
      "30              I mean I meta calls right ill go zukk      59  \n",
      "38                                     Im Meta Knight      26  \n",
      "tf.Tensor(\n",
      "[[  101  1045  5064 ...     0     0     0]\n",
      " [  101  2027  2890 ...     0     0     0]\n",
      " [  101  1045  2064 ...     0     0     0]\n",
      " ...\n",
      " [  101  1045  2903 ...     0     0     0]\n",
      " [  101 29433  7087 ...     0     0     0]\n",
      " [  101  2012  2232 ...     0     0     0]], shape=(7766, 110), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]], shape=(7766, 110), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_ids, masks = tokenize_comment(copied_df[\"Comment\"], 110)\n",
    "\n",
    "print(copied_df.head())\n",
    "print(input_ids)\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3cc44cf4-207e-4866-9995-4fd0a1446bcb)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440M/440M [03:28<00:00, 2.11MB/s] \n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God is Great! I won a lottery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 661kB/s]\n",
      "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.0/28.0 [00:00<00:00, 12.4kB/s]\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:00<00:00, 184kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "test_comment = \"God is Great! I won a lottery.\"\n",
    "\n",
    "# cleaned_test = commentCleaner(test_comment)\n",
    "print(test_comment)\n",
    "# print(cleaned_test)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "sentiment_label = predictions.item()\n",
    "\n",
    "print(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Predicted Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "sentiment_classes = ['Negative',  'Neutral', 'Positive']  # Define your sentiment classes here\n",
    "\n",
    "print(sentiment_label)\n",
    "# Map the sentiment label to the corresponding class\n",
    "sentiment = sentiment_classes[sentiment_label]\n",
    "\n",
    "print(\"Predicted Sentiment:\", sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
