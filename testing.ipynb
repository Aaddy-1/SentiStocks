{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14n378b</td>\n",
       "      <td>https://www.cnbc.com/2023/06/30/supreme-court-...</td>\n",
       "      <td>Supreme Court strikes down student loan forgiv...</td>\n",
       "      <td>3030</td>\n",
       "      <td>11023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14o2pem</td>\n",
       "      <td>https://i.redd.it/95deby9rhe9b1.jpg</td>\n",
       "      <td>When it dips to 400, buy calls</td>\n",
       "      <td>411</td>\n",
       "      <td>8190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14q1z3k</td>\n",
       "      <td>https://i.redd.it/apd3etv8xu9b1.jpg</td>\n",
       "      <td>Anybody feel this pain?</td>\n",
       "      <td>402</td>\n",
       "      <td>6677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14qgwx5</td>\n",
       "      <td>https://i.redd.it/5jtfwblyly9b1.gif</td>\n",
       "      <td>options level 4 granted</td>\n",
       "      <td>227</td>\n",
       "      <td>6223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14oy5dm</td>\n",
       "      <td>https://v.redd.it/5sx6d9pp1m9b1</td>\n",
       "      <td>When she finds out you have Tesla calls ü•µ</td>\n",
       "      <td>249</td>\n",
       "      <td>5512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14ps7jn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Continued Claims - A recession indicator with ...</td>\n",
       "      <td>164</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14nbryb</td>\n",
       "      <td>https://www.reddit.com/gallery/14nbryb</td>\n",
       "      <td>Tim Apple $AAPL 40k YOLOüçéüçè</td>\n",
       "      <td>53</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14sj6yt</td>\n",
       "      <td>https://v.redd.it/b0e8ovf3ceab1</td>\n",
       "      <td>I‚Äôm not naming any $META names, but I think I ...</td>\n",
       "      <td>21</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14onj3d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Spce doomed to fail</td>\n",
       "      <td>167</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14p3hpq</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>7 Year Streak of SPY Closing Higher in July Ve...</td>\n",
       "      <td>57</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                                url  \\\n",
       "0   14n378b  https://www.cnbc.com/2023/06/30/supreme-court-...   \n",
       "1   14o2pem                https://i.redd.it/95deby9rhe9b1.jpg   \n",
       "2   14q1z3k                https://i.redd.it/apd3etv8xu9b1.jpg   \n",
       "3   14qgwx5                https://i.redd.it/5jtfwblyly9b1.gif   \n",
       "4   14oy5dm                    https://v.redd.it/5sx6d9pp1m9b1   \n",
       "..      ...                                                ...   \n",
       "95  14ps7jn  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "96  14nbryb             https://www.reddit.com/gallery/14nbryb   \n",
       "97  14sj6yt                    https://v.redd.it/b0e8ovf3ceab1   \n",
       "98  14onj3d  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "99  14p3hpq  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "\n",
       "                                                Title  Total Comments  Score  \n",
       "0   Supreme Court strikes down student loan forgiv...            3030  11023  \n",
       "1                      When it dips to 400, buy calls             411   8190  \n",
       "2                             Anybody feel this pain?             402   6677  \n",
       "3                             options level 4 granted             227   6223  \n",
       "4           When she finds out you have Tesla calls ü•µ             249   5512  \n",
       "..                                                ...             ...    ...  \n",
       "95  Continued Claims - A recession indicator with ...             164    158  \n",
       "96                         Tim Apple $AAPL 40k YOLOüçéüçè              53    162  \n",
       "97  I‚Äôm not naming any $META names, but I think I ...              21    161  \n",
       "98                                Spce doomed to fail             167    148  \n",
       "99  7 Year Streak of SPY Closing Higher in July Ve...              57    152  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"Top_This_Week.csv\")\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\"Tesla\" : [\"$TSLA\", \"TSLA\", \"tesla\", \"elon musk\", \"musk\"],\n",
    "            \"Apple\" : [\"$AAPL\", \"AAPL\", \"apple\"], \n",
    "            \"Nvidia\" : [\"$NVDA\", \"NVDA\", \"nvidia\"], \n",
    "            \"Google\" : [\"$GOOGL\", \"GOOGL\", \"google\", \"alphabet\"],\n",
    "            \"Amazon\" : [\"$AMZN\", \"AMZN\", \"amazon\"],\n",
    "            \"Microsoft\" : [\"$MSFT\", \"MSFT\", \"microsoft\"],\n",
    "            \"Meta\" : [\"$META\", \"META\", \"meta\", \"instagram\", \"facebook\"]\n",
    "        }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40703"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv(\"PastWeekComments.csv\")\n",
    "len(comments.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Filtering our dataframe to get the comments which contain our required keywords\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mask \u001b[39m=\u001b[39m comments[\u001b[39m'\u001b[39m\u001b[39mComment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(keywords), case\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Apply the mask to filter the dataframe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m filtered_df \u001b[39m=\u001b[39m comments[mask]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keywords' is not defined"
     ]
    }
   ],
   "source": [
    "# Filtering our dataframe to get the comments which contain our required keywords\n",
    "mask = comments['Comment'].str.contains('|'.join(keywords), case=False)\n",
    "\n",
    "# Apply the mask to filter the dataframe\n",
    "filtered_df = comments[mask]\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"Length\"].describe()\n",
    "filtered_df.to_csv(\"Filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I somehow don't think that this is what any Te...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>They‚Äôre going to fake fight in the metaverse</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I can't wait for Tesla fan boys to lose all th...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I mean I have meta calls right now so i'll go ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I'm all in on Meta Knight.</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                  Title                 Date  \\\n",
       "6   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "16  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "26  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "30  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "38  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "\n",
       "                                              Comment  Length  \n",
       "6   I somehow don't think that this is what any Te...     134  \n",
       "16       They‚Äôre going to fake fight in the metaverse      44  \n",
       "26  I can't wait for Tesla fan boys to lose all th...      76  \n",
       "30  I mean I have meta calls right now so i'll go ...      59  \n",
       "38                         I'm all in on Meta Knight.      26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\"Tesla\" : [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\"],\n",
    "            \"Apple\" : [\"$aapl\", \"aapl\", \"apple\", \"mac\"], \n",
    "            \"Nvidia\" : [\"$nvda\", \"nvda\", \"nvidia\"], \n",
    "            \"Google\" : [\"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\"],\n",
    "            \"Amazon\" : [\"$amzn\", \"amzn\", \"amazon\", \"aws\"],\n",
    "            \"Microsoft\" : [\"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\"],\n",
    "            \"Meta\" : [\"$meta\", \"meta\", \"instagram\", \"facebook\"]\n",
    "        }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords2 = [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\", \n",
    "             \"$aapl\", \"aapl\", \"apple\", \"mac\",\n",
    "             \"$nvda\", \"nvda\", \"nvidia\",\n",
    "             \"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\",\n",
    "             \"$amzn\", \"amzn\", \"amazon\", \"aws\",\n",
    "             \"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\",\n",
    "             \"$meta\", \"meta\", \"instagram\", \"facebook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Length</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>\\*as far as I know, Zuck been training BJJ fo...</td>\n",
       "      <td>172</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I somehow don't think that this is what any Te...</td>\n",
       "      <td>134</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>They‚Äôre going to fake fight in the metaverse</td>\n",
       "      <td>44</td>\n",
       "      <td>Meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>I can't wait for Tesla fan boys to lose all th...</td>\n",
       "      <td>76</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14hdunm</td>\n",
       "      <td>Choose Your Fighter üö®</td>\n",
       "      <td>2023-06-23 23:58:45</td>\n",
       "      <td>Musk inverse zuck double reverse musk</td>\n",
       "      <td>37</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Post ID                  Title                 Date  \\\n",
       "1   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "6   14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "16  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "26  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "27  14hdunm  Choose Your Fighter üö®  2023-06-23 23:58:45   \n",
       "\n",
       "                                              Comment  Length Keyword  \n",
       "1    \\*as far as I know, Zuck been training BJJ fo...     172  Tesla   \n",
       "6   I somehow don't think that this is what any Te...     134  Tesla   \n",
       "16       They‚Äôre going to fake fight in the metaverse      44   Meta   \n",
       "26  I can't wait for Tesla fan boys to lose all th...      76  Tesla   \n",
       "27              Musk inverse zuck double reverse musk      37  Tesla   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = comments.copy()\n",
    "\n",
    "detected_keywords = []\n",
    "\n",
    "filtered_df = df[df['Comment'].str.contains('|'.join(keywords2), case = False)]\n",
    "\n",
    "# Add an extra column to the filtered dataframe that indicates which keyword was present in that comment\n",
    "def keyWordBuilder(comment):\n",
    "    returnString = \"\"\n",
    "    for keyword in keywords2:\n",
    "        if keyword in comment.lower():\n",
    "            for key in keywords:\n",
    "                if keyword in keywords[key]:\n",
    "                    if key not in returnString:\n",
    "                        returnString += key + ' '\n",
    "    if returnString == \"\":\n",
    "        return \"None\"\n",
    "    return returnString\n",
    "\n",
    "keyWordList = filtered_df['Comment'].apply(keyWordBuilder)\n",
    "\n",
    "filtered_df = filtered_df.assign(Keyword = keyWordList)\n",
    "\n",
    "# Print the filtered dataframe\n",
    "filtered_df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional, Flatten, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def classification_model():\n",
    "    # Building our model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(18364, 256, input_length = 235))\n",
    "    model.add(SpatialDropout1D(0.5))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=128, dropout=0.6)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "checkpoint_path = \"final1/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# Create a ModelCheckpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback to stop training if validation loss doesn't improve\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,  # Number of epochs with no improvement after which training will stop\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "class customModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, batch_size):\n",
    "        self.model_fn = classification_model()\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.model_fn\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.model.fit(X, y, epochs = 7, batch_size=self.batch_size, callbacks = [checkpoint_callback, early_stopping_callback], verbose = 1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "def commentCleaner(comments):\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        # Remove special symbols, emojis, reddit username mentions, and hyperlinks\n",
    "        comment = re.sub(r\"[^\\w\\s]|http\\S+|www\\S+|u/[A-Za-z0-9_-]+\", \"\", comment)\n",
    "        comment = comment.lower()\n",
    "        # Tokenize the comment\n",
    "        tokens = comment.split()\n",
    "        # tokens = comment.split(' ')\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Join the tokens back into a single string\n",
    "        cleaned_comment = \" \".join(tokens)\n",
    "        cleaned_comments.append(cleaned_comment)   \n",
    "    return cleaned_comments\n",
    "\n",
    "\n",
    "    \n",
    "def tokenizeComments(comments, tokenizer):\n",
    "    # print(\"Comments recieved for tokenization: \")\n",
    "    # print(comments)\n",
    "    # print(\"Fitted tokenizer to sample texts\")\n",
    "    tokenized_comments = tokenizer.texts_to_sequences(comments)\n",
    "    # print(\"Converted to sequences\")\n",
    "    tokenized_comments = pad_sequences(tokenized_comments, 235)\n",
    "    # print(\"Padded succesfully\")\n",
    "    # print(tokenized_comments)\n",
    "    return tokenized_comments\n",
    "\n",
    "class textTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # print(\"Starting fitting\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # print(\"Starting transform\")\n",
    "        # print(X)\n",
    "        # tokenizerFinal = Tokenizer(num_words=1000, split=' ') \n",
    "        # print(cleaned_data['Sentence'].values)\n",
    "        # tokenizerFinal.fit_on_texts(cleaned_data['Sentence'].values)\n",
    "        X_cleaned = commentCleaner(X)\n",
    "        # print(\"Cleaned comments\")\n",
    "        # print(\"Starting tokenization\")\n",
    "        X_tokenized = tokenizeComments(X_cleaned, self.tokenizer)\n",
    "        # print(\"Tokenized\")\n",
    "        # print(\"Ending transform\")\n",
    "\n",
    "        return X_tokenized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "def load_pipeline_keras(cleaner, model, tokenizer, folder_name=\"model\"):\n",
    "    cleaner = pickle.load(open(cleaner,'rb'))\n",
    "    tokenizerFinal = pickle.load(open(tokenizer,'rb'))\n",
    "    model = keras.models.load_model(model)\n",
    "    cleaner.tokenizer = tokenizerFinal\n",
    "    # classifier = KerasClassifier(model=build_model, epochs=1, batch_size=10, verbose=1)\n",
    "    # classifier.classes_ = pickle.load(open(folder_name+'/'+classes,'rb'))\n",
    "    # classifier.model = build_model\n",
    "    # build_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return Pipeline([\n",
    "        ('textTransformer', cleaner),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    classifier = load_pipeline_keras('/Users/aadeesh/redditSentiment/server/model/classifier/textTransformer.pkl', \n",
    "                    '/Users/aadeesh/redditSentiment/server/model/classifier/model.h5', \n",
    "                    '/Users/aadeesh/redditSentiment/server/model/classifier/tokenizer.pkl', \n",
    "                    'server/model/classifier')\n",
    "    return classifier\n",
    "\n",
    "classifier = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 11s 110ms/step\n"
     ]
    }
   ],
   "source": [
    "comments = filtered_df.Comment\n",
    "preds = classifier.predict(comments)\n",
    "\n",
    "sentiments = np.argmax(preds, axis = 1)\n",
    "# preds\n",
    "\n",
    "filtered_df = filtered_df.assign(Sentiment = sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDates = pd.to_datetime(filtered_df['Date'])\n",
    "newDates = newDates.dt.date\n",
    "filtered_df = filtered_df.assign(Date = newDates)\n",
    "filtered_df = filtered_df.sort_values(by='Date', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'06-08': 32,\n",
       " '06-09': 12,\n",
       " '06-10': 5,\n",
       " '06-11': 14,\n",
       " '06-12': 7,\n",
       " '06-13': 45,\n",
       " '06-14': 13,\n",
       " '06-15': 28,\n",
       " '06-16': 27,\n",
       " '06-17': -8,\n",
       " '06-18': -2,\n",
       " '06-19': -1,\n",
       " '06-20': 25,\n",
       " '06-21': 2,\n",
       " '06-22': 4,\n",
       " '06-23': -2,\n",
       " '06-24': 9,\n",
       " '06-25': 1,\n",
       " '06-26': 1,\n",
       " '06-27': 9,\n",
       " '06-28': 4,\n",
       " '06-29': 3,\n",
       " '06-30': 8,\n",
       " '07-01': 13,\n",
       " '07-02': 0,\n",
       " '07-03': 17,\n",
       " '07-04': 10,\n",
       " '07-05': 5,\n",
       " '07-06': 8,\n",
       " '07-07': 10}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rows = filtered_df[filtered_df['Keyword'].str.contains('tesla', case=False)]\n",
    "# filtered_rows['Date'] = pd.to_datetime(filtered_rows['Date'])\n",
    "\n",
    "# # Extract only the date part from the 'Date' column\n",
    "# filtered_rows['Date'] = filtered_rows['Date'].dt.date\n",
    "# print(filtered_rows.head())\n",
    "tesla_df = {}\n",
    "\n",
    "\n",
    "for i, j, k in zip(filtered_rows.Date, filtered_rows.Keyword, filtered_rows.Sentiment):\n",
    "    date_string = i.strftime('%m-%d')\n",
    "    val = 1\n",
    "    if k == 0:\n",
    "        val = -1\n",
    "    if date_string not in tesla_df.keys():\n",
    "        tesla_df[date_string] = 0\n",
    "        tesla_df[date_string] += val\n",
    "    else:\n",
    "        tesla_df[date_string] += val\n",
    "\n",
    "tesla_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframeProcessor(df, classifier):\n",
    "\n",
    "    keywords = {\"Tesla\" : [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\"],\n",
    "            \"Apple\" : [\"$aapl\", \"aapl\", \"apple\", \"mac\"], \n",
    "            \"Nvidia\" : [\"$nvda\", \"nvda\", \"nvidia\"], \n",
    "            \"Google\" : [\"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\"],\n",
    "            \"Amazon\" : [\"$amzn\", \"amzn\", \"amazon\", \"aws\"],\n",
    "            \"Microsoft\" : [\"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\"],\n",
    "            \"Meta\" : [\"$meta\", \"meta\", \"instagram\", \"facebook\"]\n",
    "        }\n",
    "    keywords2 = [\"$tsla\", \"tsla\", \"tesla\", \"elon musk\", \"musk\", \n",
    "             \"$aapl\", \"aapl\", \"apple\", \"mac\",\n",
    "             \"$nvda\", \"nvda\", \"nvidia\",\n",
    "             \"$googl\", \"googl\", \"google\", \"alphabet\", \"bard\",\n",
    "             \"$amzn\", \"amzn\", \"amazon\", \"aws\",\n",
    "             \"$msft\", \"msft\", \"microsoft\", \"windows\", \"azure\",\n",
    "             \"$meta\", \"meta\", \"instagram\", \"facebook\"\n",
    "        ]\n",
    "\n",
    "    filtered_df = df[df['Comment'].str.contains('|'.join(keywords2), case = False)]\n",
    "\n",
    "    # Add an extra column to the filtered dataframe that indicates which keyword was present in that comment\n",
    "    def keyWordBuilder(comment):\n",
    "        returnString = \"\"\n",
    "        for keyword in keywords2:\n",
    "            if keyword in comment.lower():\n",
    "                for key in keywords:\n",
    "                    if keyword in keywords[key]:\n",
    "                        if key not in returnString:\n",
    "                            returnString += key + ' '\n",
    "        if returnString == \"\":\n",
    "            return \"None\"\n",
    "        return returnString\n",
    "\n",
    "    keyWordList = filtered_df['Comment'].apply(keyWordBuilder)\n",
    "\n",
    "    filtered_df = filtered_df.assign(Keyword = keyWordList)\n",
    "\n",
    "    newDates = pd.to_datetime(filtered_df['Date'])\n",
    "    newDates = newDates.dt.date\n",
    "    filtered_df = filtered_df.assign(Date = newDates)\n",
    "    filtered_df = filtered_df.sort_values(by='Date', ascending=True)\n",
    "\n",
    "    comments = filtered_df.Comment\n",
    "    preds = classifier.predict(comments)\n",
    "\n",
    "    sentiments = np.argmax(preds, axis = 1)\n",
    "    # preds\n",
    "\n",
    "    filtered_df = filtered_df.assign(Sentiment = sentiments)\n",
    "\n",
    "    return filtered_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'06-08': 40, '06-09': 34, '06-10': 27, '06-11': 39, '06-12': 26, '06-13': 68, '06-14': 25, '06-15': 54, '06-16': 37, '06-17': 13, '06-18': 8, '06-19': 1, '06-20': 46, '06-21': 9, '06-22': 34, '06-23': 56, '06-24': 13, '06-25': 9, '06-26': 1, '06-27': 11, '06-28': 22, '06-29': 12, '06-30': 12, '07-01': 27, '07-02': 27, '07-03': 60, '07-04': 19, '07-05': 15, '07-06': 46, '07-07': 22}\n",
      "{'06-08': 3, '06-09': 3, '06-10': 2, '06-11': 10, '06-12': 19, '06-13': 18, '06-14': 7, '06-15': 16, '06-16': 28, '06-17': 4, '06-18': 40, '06-19': 2, '06-20': 15, '06-21': 3, '06-22': 5, '06-23': 7, '06-24': 1, '06-25': 4, '06-26': 9, '06-27': 6, '06-28': 7, '06-29': 2, '06-30': 58, '07-01': 33, '07-02': 20, '07-03': 24, '07-04': 4, '07-05': 3, '07-06': 12, '07-07': 1}\n",
      "{'06-08': 7, '06-09': 2, '06-10': 32, '06-11': 18, '06-12': 20, '06-13': 38, '06-14': 54, '06-15': 62, '06-16': 26, '06-17': 2, '06-18': 5, '06-19': 6, '06-20': 18, '06-21': 14, '06-22': 21, '06-23': 0, '06-24': 2, '06-25': 6, '06-26': 2, '06-27': 20, '06-28': 30, '06-29': 1, '06-30': 4, '07-01': 23, '07-02': 36, '07-03': 6, '07-04': 26, '07-05': 2, '07-06': 13, '07-07': 0}\n",
      "{'06-08': 0, '06-09': 1, '06-10': 3, '06-11': 6, '06-12': 4, '06-13': 9, '06-14': 20, '06-15': 6, '06-16': 10, '06-17': 2, '06-18': 7, '06-19': 0, '06-20': 1, '06-21': 2, '06-22': 0, '06-23': 2, '06-24': 1, '06-25': 1, '06-26': 0, '06-27': 3, '06-28': 5, '06-29': 0, '06-30': 1, '07-01': 7, '07-02': 0, '07-03': 2, '07-04': 0, '07-05': 2, '07-06': 1, '07-07': 1}\n",
      "{'06-08': 0, '06-09': 4, '06-10': 5, '06-11': 3, '06-12': 5, '06-13': 11, '06-14': 6, '06-15': 2, '06-16': 5, '06-17': 0, '06-18': 6, '06-19': 3, '06-20': 0, '06-21': 1, '06-22': 2, '06-23': 4, '06-24': 5, '06-25': 0, '06-26': 0, '06-27': 7, '06-28': 5, '06-29': 2, '06-30': 2, '07-01': 2, '07-02': 1, '07-03': 0, '07-04': 3, '07-05': 2, '07-06': 2, '07-07': 1}\n",
      "{'06-08': 0, '06-09': 0, '06-10': 1, '06-11': 5, '06-12': 5, '06-13': 14, '06-14': 4, '06-15': 4, '06-16': 8, '06-17': 0, '06-18': 3, '06-19': 0, '06-20': 1, '06-21': 0, '06-22': 1, '06-23': 0, '06-24': 1, '06-25': 1, '06-26': 0, '06-27': 1, '06-28': 2, '06-29': 0, '06-30': 3, '07-01': 4, '07-02': 0, '07-03': 0, '07-04': 0, '07-05': 1, '07-06': 0, '07-07': 1}\n",
      "{'06-08': 0, '06-09': 3, '06-10': 6, '06-11': 4, '06-12': 3, '06-13': 4, '06-14': 3, '06-15': 5, '06-16': 9, '06-17': 2, '06-18': 6, '06-19': 4, '06-20': 2, '06-21': 0, '06-22': 3, '06-23': 16, '06-24': 0, '06-25': 2, '06-26': 0, '06-27': 1, '06-28': 4, '06-29': 0, '06-30': 4, '07-01': 4, '07-02': 1, '07-03': 5, '07-04': 4, '07-05': 35, '07-06': 19, '07-07': 16}\n"
     ]
    }
   ],
   "source": [
    "def jsonBuilder(filtered_df):\n",
    "    # filtered_rows = filtered_df[filtered_df['Keyword'].str.contains('tesla', case=False)]\n",
    "    # filtered_rows['Date'] = pd.to_datetime(filtered_rows['Date'])\n",
    "\n",
    "    # # Extract only the date part from the 'Date' column\n",
    "    # filtered_rows['Date'] = filtered_rows['Date'].dt.date\n",
    "    # print(filtered_rows.head())\n",
    "    tesla_df, apple_df, nvda_df, google_df, amzn_df, msft_df, meta_df = {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    done = []\n",
    "    for i in (filtered_df.Date):\n",
    "        date_string = i.strftime('%m-%d')\n",
    "        if date_string not in done:\n",
    "            tesla_df[date_string] = 0\n",
    "            apple_df[date_string] = 0\n",
    "            nvda_df[date_string] = 0\n",
    "            google_df[date_string] = 0\n",
    "            amzn_df[date_string] = 0\n",
    "            msft_df[date_string] = 0\n",
    "            meta_df[date_string] = 0\n",
    "        done.append(date_string)\n",
    "\n",
    "    for i, j, k in zip(filtered_df.Date, filtered_df.Keyword, filtered_df.Sentiment):\n",
    "        date_string = i.strftime('%m-%d')\n",
    "        val = 1\n",
    "        if k == 0:\n",
    "            val = 0\n",
    "        for keyword in j.split():\n",
    "            if keyword == \"Tesla\":\n",
    "                tesla_df[date_string] += val\n",
    "            if keyword == \"Apple\":\n",
    "                apple_df[date_string] += val\n",
    "            if keyword == \"Nvidia\":\n",
    "                nvda_df[date_string] += val\n",
    "            if keyword == \"Google\":\n",
    "                google_df[date_string] += val\n",
    "            if keyword == \"Amazon\":\n",
    "                amzn_df[date_string] += val\n",
    "            if keyword == \"Microsoft\":\n",
    "                msft_df[date_string] += val\n",
    "            if keyword == \"Meta\":\n",
    "                meta_df[date_string] += val\n",
    "    return [tesla_df, apple_df, nvda_df, google_df, amzn_df, msft_df, meta_df]\n",
    "\n",
    "l = jsonBuilder(filtered_df=filtered_df)\n",
    "for i in l:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
